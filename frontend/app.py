"""
Mini LLM Studio - Streamlit Web UI
ç”¨äº LLM å¾®è°ƒå’Œå¯¹è¯çš„ç®€å•ç•Œé¢
"""
import streamlit as st
import requests
import json
import time
from pathlib import Path
from ws_client import StreamlitWebSocketClient, format_log_message

# é…ç½®
BACKEND_URL = "http://localhost:8000"
INFERENCE_URL = "http://localhost:8001"
RAG_URL = "http://localhost:8002"

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Mini LLM Studio",
    page_icon="ğŸ¤–",
    layout="wide"
)

# åˆå§‹åŒ– session state
if 'current_run_id' not in st.session_state:
    st.session_state.current_run_id = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'selected_model' not in st.session_state:
    st.session_state.selected_model = None


def check_backend_health():
    """æ£€æŸ¥åç«¯æœåŠ¡çŠ¶æ€"""
    try:
        response = requests.get(f"{BACKEND_URL}/health", timeout=2)
        return response.status_code == 200
    except:
        return False


def check_inference_health():
    """æ£€æŸ¥æ¨ç†æœåŠ¡çŠ¶æ€"""
    try:
        response = requests.get(f"{INFERENCE_URL}/health", timeout=2)
        return response.status_code == 200
    except:
        return False


def start_training(data_file, dataset_name, params):
    """å¯åŠ¨è®­ç»ƒä»»åŠ¡"""
    try:
        files = None
        data = {
            "base_model": params["base_model"],
            "lora_r": params["lora_r"],
            "lora_alpha": params["lora_alpha"],
            "num_epochs": params["num_epochs"],
            "batch_size": params["batch_size"],
            "learning_rate": params["learning_rate"],
        }
        
        # å¦‚æœä½¿ç”¨ HuggingFace æ•°æ®é›†
        if dataset_name:
            data["dataset_name"] = dataset_name
            data["num_samples"] = params.get("num_samples")
        
        # å¦‚æœä¸Šä¼ äº†æ–‡ä»¶
        if data_file is not None:
            files = {"data_file": data_file}
        
        response = requests.post(
            f"{BACKEND_URL}/start_finetune",
            files=files,
            data=data
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": response.text}
    except Exception as e:
        return {"error": str(e)}


def get_task_status(run_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        response = requests.get(f"{BACKEND_URL}/status/{run_id}")
        if response.status_code == 200:
            return response.json()
        return None
    except:
        return None


def get_all_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡"""
    try:
        response = requests.get(f"{BACKEND_URL}/tasks")
        if response.status_code == 200:
            return response.json()["tasks"]
        return []
    except:
        return []


def get_logs(run_id, log_type="stderr"):
    """è·å–æ—¥å¿—"""
    try:
        response = requests.get(f"{BACKEND_URL}/logs/{run_id}?log_type={log_type}")
        if response.status_code == 200:
            return response.json()["logs"]
        return "æ— æ³•è·å–æ—¥å¿—"
    except:
        return "æ— æ³•è·å–æ—¥å¿—"


def stop_task(run_id):
    """åœæ­¢ä»»åŠ¡"""
    try:
        response = requests.post(f"{BACKEND_URL}/stop/{run_id}")
        return response.status_code == 200
    except:
        return False


def get_available_models():
    """è·å–å¯ç”¨çš„æ¨ç†æ¨¡å‹"""
    try:
        response = requests.get(f"{INFERENCE_URL}/models")
        if response.status_code == 200:
            return response.json()["models"]
        return []
    except:
        return []


def load_inference_model(model_id):
    """åŠ è½½æ¨ç†æ¨¡å‹"""
    try:
        response = requests.post(f"{INFERENCE_URL}/load_model", params={"model_id": model_id})
        return response.status_code == 200
    except:
        return False


def chat_with_model(model_id, message, temperature=0.7, max_length=256):
    """ä¸æ¨¡å‹å¯¹è¯"""
    try:
        payload = {
            "model_id": model_id,
            "message": message,
            "temperature": temperature,
            "max_length": max_length
        }
        response = requests.post(f"{INFERENCE_URL}/chat", json=payload)
        if response.status_code == 200:
            return response.json()["response"]
        return None
    except:
        return None


def chat_with_rag(model_id, message, temperature=0.7, max_length=256, top_k=3):
    """ä½¿ç”¨ RAG å¢å¼ºçš„å¯¹è¯"""
    try:
        payload = {
            "model_id": model_id,
            "message": message,
            "temperature": temperature,
            "max_length": max_length,
            "use_rag": True,
            "rag_top_k": top_k
        }
        response = requests.post(f"{INFERENCE_URL}/chat", json=payload)
        if response.status_code == 200:
            data = response.json()
            return data["response"], data.get("rag_sources", [])
        return None, []
    except:
        return None, []


def upload_documents_to_rag(files):
    """ä¸Šä¼ æ–‡æ¡£åˆ° RAG çŸ¥è¯†åº“"""
    try:
        files_data = [("files", (file.name, file, "application/octet-stream")) for file in files]
        response = requests.post(f"{RAG_URL}/upload_files", files=files_data)
        return response.json() if response.status_code == 200 else None
    except:
        return None


def get_rag_stats():
    """è·å– RAG çŸ¥è¯†åº“ç»Ÿè®¡"""
    try:
        response = requests.get(f"{RAG_URL}/stats")
        return response.json() if response.status_code == 200 else None
    except:
        return None


def clear_rag_knowledge_base():
    """æ¸…ç©º RAG çŸ¥è¯†åº“"""
    try:
        response = requests.post(f"{RAG_URL}/clear")
        return response.status_code == 200
    except:
        return False


# ==================== ä¸»ç•Œé¢ ====================

st.title("ğŸ¤– Mini LLM Studio")
st.markdown("è½»é‡çº§ LLM å¾®è°ƒå¹³å°")

# æ£€æŸ¥åç«¯çŠ¶æ€
if not check_backend_health():
    st.error("âš ï¸ åç«¯æœåŠ¡æœªè¿è¡Œï¼è¯·å…ˆå¯åŠ¨: `python api.py`")
    st.stop()

st.success("âœ… åç«¯æœåŠ¡æ­£å¸¸")

# åˆ›å»ºæ ‡ç­¾é¡µ
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“š è®­ç»ƒå¾®è°ƒ", "ğŸ“Š ä»»åŠ¡ç®¡ç†", "ğŸ“ RAG çŸ¥è¯†åº“", "ğŸ’¬ æ¨¡å‹å¯¹è¯"])

# ==================== Tab 1: è®­ç»ƒå¾®è°ƒ ====================
with tab1:
    st.header("1. é…ç½®è®­ç»ƒä»»åŠ¡")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("æ•°æ®æº")
        data_source = st.radio(
            "é€‰æ‹©æ•°æ®æ¥æº",
            ["HuggingFace æ•°æ®é›†", "ä¸Šä¼  JSONL æ–‡ä»¶"],
            help="å¯ä»¥ä½¿ç”¨ HuggingFace ä¸Šçš„æ•°æ®é›†æˆ–ä¸Šä¼ è‡ªå·±çš„æ•°æ®"
        )
        
        dataset_name = None
        data_file = None
        
        if data_source == "HuggingFace æ•°æ®é›†":
            dataset_name = st.text_input(
                "æ•°æ®é›†åç§°",
                value="tatsu-lab/alpaca",
                help="ä¾‹å¦‚: tatsu-lab/alpaca"
            )
            num_samples = st.number_input(
                "ä½¿ç”¨æ ·æœ¬æ•°",
                min_value=100,
                max_value=100000,
                value=1000,
                step=100,
                help="ç•™ç©ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œå»ºè®®å…ˆç”¨å°‘é‡æ•°æ®æµ‹è¯•"
            )
        else:
            data_file = st.file_uploader(
                "ä¸Šä¼ è®­ç»ƒæ•°æ®",
                type=["jsonl"],
                help="JSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡"
            )
            num_samples = None
            
            if data_file:
                st.info(f"âœ… å·²é€‰æ‹©æ–‡ä»¶: {data_file.name}")
    
    with col2:
        st.subheader("æ¨¡å‹é…ç½®")
        base_model = st.text_input(
            "åŸºç¡€æ¨¡å‹",
            value="Qwen/Qwen2-1.5B-Instruct",
            help="HuggingFace æ¨¡å‹åç§°"
        )
        
        st.subheader("LoRA å‚æ•°")
        col_a, col_b = st.columns(2)
        with col_a:
            lora_r = st.number_input("LoRA Rank", 4, 64, 16, 4)
        with col_b:
            lora_alpha = st.number_input("LoRA Alpha", 8, 128, 32, 8)
    
    st.subheader("è®­ç»ƒå‚æ•°")
    col3, col4, col5 = st.columns(3)
    
    with col3:
        num_epochs = st.number_input("è®­ç»ƒè½®æ•°", 1, 10, 3, 1)
    with col4:
        batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", 1, 16, 4, 1)
    with col5:
        learning_rate = st.number_input(
            "å­¦ä¹ ç‡",
            min_value=0.00001,
            max_value=0.001,
            value=0.0002,
            step=0.00001,
            format="%.5f"
        )
    
    # å¯åŠ¨è®­ç»ƒæŒ‰é’®
    st.markdown("---")
    if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary", use_container_width=True):
        # éªŒè¯è¾“å…¥
        if data_source == "ä¸Šä¼  JSONL æ–‡ä»¶" and data_file is None:
            st.error("âŒ è¯·å…ˆä¸Šä¼ è®­ç»ƒæ•°æ®æ–‡ä»¶")
        elif data_source == "HuggingFace æ•°æ®é›†" and not dataset_name:
            st.error("âŒ è¯·è¾“å…¥æ•°æ®é›†åç§°")
        else:
            with st.spinner("æ­£åœ¨å¯åŠ¨è®­ç»ƒä»»åŠ¡..."):
                params = {
                    "base_model": base_model,
                    "lora_r": lora_r,
                    "lora_alpha": lora_alpha,
                    "num_epochs": num_epochs,
                    "batch_size": batch_size,
                    "learning_rate": learning_rate,
                    "num_samples": num_samples,
                }
                
                result = start_training(data_file, dataset_name, params)
                
                if "error" in result:
                    st.error(f"âŒ å¯åŠ¨å¤±è´¥: {result['error']}")
                else:
                    st.success(f"âœ… è®­ç»ƒå·²å¯åŠ¨ï¼Run ID: {result['run_id']}")
                    st.session_state.current_run_id = result['run_id']
                    time.sleep(1)
                    st.rerun()

# ==================== Tab 2: ä»»åŠ¡ç®¡ç† ====================
with tab2:
    st.header("ğŸ“Š è®­ç»ƒä»»åŠ¡ç®¡ç†")
    
    # åˆ·æ–°æŒ‰é’®
    if st.button("ğŸ”„ åˆ·æ–°", key="refresh_tasks"):
        st.rerun()
    
    # è·å–æ‰€æœ‰ä»»åŠ¡
    tasks = get_all_tasks()
    
    if not tasks:
        st.info("æš‚æ— è®­ç»ƒä»»åŠ¡")
    else:
        st.markdown(f"**å…± {len(tasks)} ä¸ªä»»åŠ¡**")
        
        # æ˜¾ç¤ºä»»åŠ¡åˆ—è¡¨
        for task in tasks:
            run_id = task["run_id"]
            status = task["status"]
            progress = task.get("progress", 0)
            
            # çŠ¶æ€å›¾æ ‡
            status_icon = {
                "running": "ğŸ”„",
                "completed": "âœ…",
                "failed": "âŒ",
                "stopped": "â¸ï¸",
                "starting": "â³"
            }.get(status, "â“")
            
            with st.expander(f"{status_icon} {run_id[:8]}... - {status} ({progress}%)"):
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.write(f"**æ•°æ®æº:** {task.get('data_source', 'N/A')}")
                    st.write(f"**åŸºç¡€æ¨¡å‹:** {task.get('base_model', 'N/A')}")
                    st.write(f"**åˆ›å»ºæ—¶é—´:** {task.get('created_at', 'N/A')}")
                    st.write(f"**è¿›åº¦:** {progress}%")
                    
                    # æ˜¾ç¤ºå‚æ•°
                    params = task.get('params', {})
                    if params:
                        st.write("**å‚æ•°:**")
                        st.json(params)
                
                with col2:
                    # æ“ä½œæŒ‰é’®
                    if status == "running":
                        if st.button("â¸ï¸ åœæ­¢", key=f"stop_{run_id}"):
                            if stop_task(run_id):
                                st.success("å·²å‘é€åœæ­¢å‘½ä»¤")
                                time.sleep(1)
                                st.rerun()
                    
                    if st.button("ğŸ“‹ æŸ¥çœ‹è¯¦æƒ…", key=f"detail_{run_id}"):
                        st.session_state.current_run_id = run_id
                
                # æ˜¾ç¤ºæ—¥å¿—
                if st.checkbox("æ˜¾ç¤ºæ—¥å¿—", key=f"logs_{run_id}"):
                    # é€‰æ‹©æ—¥å¿—æ¨¡å¼
                    log_mode = st.radio(
                        "æ—¥å¿—æ¨¡å¼",
                        ["å®æ—¶ WebSocket", "è½®è¯¢è·å–"],
                        key=f"log_mode_{run_id}",
                        horizontal=True
                    )
                    
                    if log_mode == "å®æ—¶ WebSocket":
                        # å®æ—¶ WebSocket æ—¥å¿—æ˜¾ç¤º
                        st.subheader("ğŸ”´ å®æ—¶æ—¥å¿—æµ")
                        
                        # åˆå§‹åŒ– WebSocket å®¢æˆ·ç«¯
                        ws_key = f"ws_client_{run_id}"
                        if ws_key not in st.session_state:
                            st.session_state[ws_key] = StreamlitWebSocketClient(BACKEND_URL, run_id)
                        
                        ws_client = st.session_state[ws_key]
                        
                        # æ—¥å¿—æ˜¾ç¤ºå®¹å™¨
                        log_container = st.container(border=True)
                        
                        # æ›´æ–°æŒ‰é’®å’Œåˆ·æ–°é—´éš”
                        col_a, col_b = st.columns([3, 1])
                        with col_a:
                            refresh_interval = st.slider(
                                "åˆ·æ–°é—´éš” (ç§’)",
                                1, 10, 2,
                                key=f"refresh_interval_{run_id}"
                            )
                        with col_b:
                            if st.button("ğŸ”„ æ‰‹åŠ¨åˆ·æ–°", key=f"manual_refresh_{run_id}"):
                                st.rerun()
                        
                        # è·å–æ—¥å¿—
                        with log_container:
                            logs = ws_client.get_logs()
                            if logs:
                                log_text = "\n".join([
                                    format_log_message(log) if isinstance(log, dict) else str(log)
                                    for log in logs[-100:]  # æ˜¾ç¤ºæœ€å 100 è¡Œ
                                ])
                                st.code(log_text, language="text")
                                st.caption(f"ğŸ“Š å…± {len(logs)} æ¡æ—¥å¿—")
                            else:
                                st.info("æš‚æ— æ—¥å¿—æ•°æ®")
                        
                        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
                        status_info = ws_client.get_status()
                        if status_info:
                            col_x, col_y = st.columns(2)
                            with col_x:
                                st.metric("ä»»åŠ¡çŠ¶æ€", status_info.get("status", "unknown"))
                            with col_y:
                                st.metric("è¿›åº¦", f"{status_info.get('progress', 0)}%")
                        
                        # è‡ªåŠ¨åˆ·æ–°ï¼ˆä½¿ç”¨ Streamlit çš„ rerun åŠŸèƒ½ï¼‰
                        st.session_state[f"last_refresh_{run_id}"] = time.time()
                    
                    else:
                        # ä¼ ç»Ÿè½®è¯¢æ–¹å¼
                        st.subheader("ğŸ“‹ æ—¥å¿—å†…å®¹")
                        log_type = st.radio(
                            "æ—¥å¿—ç±»å‹",
                            ["stderr", "stdout", "command"],
                            key=f"log_type_{run_id}",
                            horizontal=True
                        )
                        logs = get_logs(run_id, log_type)
                        st.code(logs, language="text")
                
                # å¦‚æœè®­ç»ƒå¤±è´¥ï¼Œæ˜¾ç¤ºé”™è¯¯
                if status == "failed" and "error" in task:
                    st.error(f"**é”™è¯¯:** {task['error']}")
                
                # å¦‚æœå®Œæˆï¼Œæ˜¾ç¤ºæ¨¡å‹è·¯å¾„
                if status == "completed" and "model_path" in task:
                    st.info(f"**æ¨¡å‹è·¯å¾„:** {task['model_path']}")

# ==================== Tab 3: RAG çŸ¥è¯†åº“ ====================
with tab3:
    st.header("ğŸ“ RAG çŸ¥è¯†åº“ç®¡ç†")
    
    st.markdown("""
    RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) å…è®¸æ¨¡å‹åŸºäºä½ ä¸Šä¼ çš„æ–‡æ¡£æ¥å›ç­”é—®é¢˜ã€‚
    """)
    
    # çŸ¥è¯†åº“ç»Ÿè®¡
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ä¸Šä¼ æ–‡æ¡£")
        
        uploaded_files = st.file_uploader(
            "é€‰æ‹©æ–‡ä»¶",
            type=["txt", "json", "jsonl"],
            accept_multiple_files=True,
            help="æ”¯æŒ .txt, .json, .jsonl æ ¼å¼"
        )
        
        if uploaded_files:
            if st.button("ğŸ“¤ ä¸Šä¼ åˆ°çŸ¥è¯†åº“", type="primary"):
                with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                    result = upload_documents_to_rag(uploaded_files)
                    
                    if result:
                        st.success(f"âœ… {result['message']}")
                        st.json(result['stats'])
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error("âŒ ä¸Šä¼ å¤±è´¥")
    
    with col2:
        st.subheader("çŸ¥è¯†åº“çŠ¶æ€")
        
        stats = get_rag_stats()
        if stats:
            st.metric("æ–‡æ¡£å—æ•°", stats['total_chunks'])
            st.metric("æ–‡æ¡£æ•°", stats['total_documents'])
            st.metric("å‘é‡ç»´åº¦", stats['embedding_dim'])
        else:
            st.warning("âš ï¸ RAG æœåŠ¡æœªè¿è¡Œ")
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“", type="secondary"):
            if clear_rag_knowledge_base():
                st.success("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
                time.sleep(1)
                st.rerun()
    
    st.markdown("---")
    
    # æµ‹è¯•æ£€ç´¢
    st.subheader("ğŸ” æµ‹è¯•æ£€ç´¢")
    
    test_query = st.text_input("è¾“å…¥æµ‹è¯•æŸ¥è¯¢", placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
    
    if st.button("ğŸ” æœç´¢") and test_query:
        try:
            response = requests.post(
                f"{RAG_URL}/search",
                json={"query": test_query, "top_k": 5}
            )
            
            if response.status_code == 200:
                results = response.json()["results"]
                
                if results:
                    st.write(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š")
                    
                    for i, result in enumerate(results, 1):
                        with st.expander(f"ç»“æœ {i} (ç›¸ä¼¼åº¦åˆ†æ•°: {result['score']:.4f})"):
                            st.write("**å†…å®¹:**")
                            st.write(result['content'])
                            
                            if result.get('metadata'):
                                st.write("**å…ƒæ•°æ®:**")
                                st.json(result['metadata'])
                else:
                    st.info("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        except:
            st.error("âŒ æ£€ç´¢å¤±è´¥ï¼Œè¯·ç¡®ä¿ RAG æœåŠ¡æ­£åœ¨è¿è¡Œ")

# ==================== Tab 4: æ¨¡å‹å¯¹è¯ ====================
with tab4:
    st.header("ğŸ’¬ ä¸è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è¯")
    
    # æ£€æŸ¥æ¨ç†æœåŠ¡
    if not check_inference_health():
        st.warning("âš ï¸ æ¨ç†æœåŠ¡æœªè¿è¡Œï¼è¯·å…ˆå¯åŠ¨: `python inference_api.py --port 8001`")
        st.info("æ¨ç†æœåŠ¡è¿è¡Œåœ¨ç«¯å£ 8001ï¼Œç”¨äºåŠ è½½æ¨¡å‹å¹¶æä¾›èŠå¤©åŠŸèƒ½")
    else:
        st.success("âœ… æ¨ç†æœåŠ¡æ­£å¸¸")
        
        # è·å–å¯ç”¨æ¨¡å‹
        available_models = get_available_models()
        
        if not available_models:
            st.info("æš‚æ— å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆå®Œæˆè®­ç»ƒä»»åŠ¡")
        else:
            # æ¨¡å‹é€‰æ‹©
            col1, col2 = st.columns([2, 1])
            
            with col1:
                model_options = [
                    f"{m['model_id'][:8]}... ({m['base_model']})" 
                    for m in available_models
                ]
                selected_idx = st.selectbox(
                    "é€‰æ‹©æ¨¡å‹",
                    range(len(model_options)),
                    format_func=lambda x: model_options[x],
                    key="model_selector"
                )
                
                selected_model = available_models[selected_idx]
                model_id = selected_model['model_id']
                
                # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
                st.write(f"**æ¨¡å‹ ID:** {model_id}")
                st.write(f"**åŸºç¡€æ¨¡å‹:** {selected_model['base_model']}")
                st.write(f"**çŠ¶æ€:** {'ğŸŸ¢ å·²åŠ è½½' if selected_model['loaded'] else 'âšª æœªåŠ è½½'}")
            
            with col2:
                # åŠ è½½/å¸è½½æŒ‰é’®
                if not selected_model['loaded']:
                    if st.button("ğŸ“¦ åŠ è½½æ¨¡å‹", type="primary", use_container_width=True):
                        with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
                            if load_inference_model(model_id):
                                st.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
                                time.sleep(1)
                                st.rerun()
                            else:
                                st.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
                else:
                    st.success("âœ… æ¨¡å‹å·²å°±ç»ª")
                
                # RAG å¼€å…³
                use_rag = st.checkbox("ğŸ” ä½¿ç”¨ RAG", help="ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯")
                
                if use_rag:
                    rag_top_k = st.slider("æ£€ç´¢æ–‡æ¡£æ•°", 1, 10, 3)
                else:
                    rag_top_k = 3
                
                # å‚æ•°è®¾ç½®
                temperature = st.slider("Temperature", 0.1, 1.0, 0.7, 0.1, key="temp")
                max_length = st.slider("æœ€å¤§é•¿åº¦", 64, 512, 256, 64, key="max_len")
            
            st.markdown("---")
            
            # èŠå¤©ç•Œé¢
            if selected_model['loaded'] or st.session_state.get('force_chat'):
                st.subheader("ğŸ’¬ å¼€å§‹å¯¹è¯")
                
                # æ˜¾ç¤ºèŠå¤©å†å²
                chat_container = st.container()
                with chat_container:
                    for msg in st.session_state.chat_history:
                        if msg['role'] == 'user':
                            st.chat_message("user").write(msg['content'])
                        else:
                            st.chat_message("assistant").write(msg['content'])
                
                # è¾“å…¥æ¡†
                user_input = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜...")
                
                if user_input:
                    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                    st.session_state.chat_history.append({
                        'role': 'user',
                        'content': user_input
                    })
                    
                    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
                    with chat_container:
                        st.chat_message("user").write(user_input)
                    
                    # ç”Ÿæˆå›å¤
                    with st.spinner("æ­£åœ¨æ€è€ƒ..."):
                        if use_rag:
                            response, rag_sources = chat_with_rag(
                                model_id,
                                user_input,
                                temperature=temperature,
                                max_length=max_length,
                                top_k=rag_top_k
                            )
                        else:
                            response = chat_with_model(
                                model_id,
                                user_input,
                                temperature=temperature,
                                max_length=max_length
                            )
                            rag_sources = []
                    
                    if response:
                        # æ·»åŠ  AI å›å¤
                        st.session_state.chat_history.append({
                            'role': 'assistant',
                            'content': response,
                            'rag_sources': rag_sources if use_rag else None
                        })
                        
                        # æ˜¾ç¤º AI å›å¤
                        with chat_container:
                            st.chat_message("assistant").write(response)
                            
                            # å¦‚æœä½¿ç”¨äº† RAGï¼Œæ˜¾ç¤ºå¼•ç”¨æ¥æº
                            if use_rag and rag_sources:
                                with st.expander(f"ğŸ“š å¼•ç”¨äº† {len(rag_sources)} ä¸ªæ¥æº"):
                                    for i, source in enumerate(rag_sources, 1):
                                        st.write(f"**æ¥æº {i}:**")
                                        st.text(source['content'][:200] + "...")
                                        st.caption(f"ç›¸ä¼¼åº¦: {source['score']:.4f}")
                    else:
                        st.error("âŒ ç”Ÿæˆå›å¤å¤±è´¥")
                
                # æ¸…ç©ºå¯¹è¯
                col_a, col_b, col_c = st.columns([1, 1, 2])
                with col_a:
                    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
                        st.session_state.chat_history = []
                        st.rerun()
                
                with col_b:
                    if st.button("ğŸ’¾ å¯¼å‡ºå¯¹è¯", use_container_width=True):
                        conversation = json.dumps(
                            st.session_state.chat_history,
                            ensure_ascii=False,
                            indent=2
                        )
                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½ JSON",
                            conversation,
                            file_name="conversation.json",
                            mime="application/json"
                        )
            else:
                st.info("ğŸ‘† è¯·å…ˆåŠ è½½æ¨¡å‹ä»¥å¼€å§‹å¯¹è¯")

# ==================== ä¾§è¾¹æ  ====================
with st.sidebar:
    st.header("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
    
    # åç«¯çŠ¶æ€
    health = check_backend_health()
    if health:
        st.success("âœ… åç«¯æœåŠ¡: æ­£å¸¸")
    else:
        st.error("âŒ åç«¯æœåŠ¡: ç¦»çº¿")
    
    st.markdown(f"**åç«¯åœ°å€:** {BACKEND_URL}")
    st.markdown(f"**æ¨ç†æœåŠ¡:** {INFERENCE_URL}")
    
    # æœåŠ¡çŠ¶æ€
    inference_health = check_inference_health()
    if inference_health:
        st.success("âœ… æ¨ç†æœåŠ¡: æ­£å¸¸")
    else:
        st.warning("âš ï¸ æ¨ç†æœåŠ¡: ç¦»çº¿")
    
    # å½“å‰ä»»åŠ¡
    if st.session_state.current_run_id:
        st.markdown("---")
        st.subheader("ğŸ“ å½“å‰ä»»åŠ¡")
        run_id = st.session_state.current_run_id
        status = get_task_status(run_id)
        
        if status:
            st.write(f"**Run ID:** {run_id[:8]}...")
            st.write(f"**çŠ¶æ€:** {status['status']}")
            st.progress(status.get('progress', 0) / 100)
            
            if status['status'] == 'running':
                if st.button("ğŸ”„ è‡ªåŠ¨åˆ·æ–°", key="auto_refresh"):
                    time.sleep(2)
                    st.rerun()
    
    # å¿«æ·æ“ä½œ
    st.markdown("---")
    st.subheader("ğŸ”— å¿«æ·é“¾æ¥")
    st.markdown("[ğŸ“Š MLflow UI](http://localhost:5000)")
    st.markdown("[ğŸ“– API æ–‡æ¡£](http://localhost:8000/docs)")
    
    # ä½¿ç”¨è¯´æ˜
    st.markdown("---")
    st.subheader("ğŸ“š ä½¿ç”¨è¯´æ˜")
    st.markdown("""
    1. **è®­ç»ƒå¾®è°ƒ**: ä¸Šä¼ æ•°æ®æˆ–é€‰æ‹© HF æ•°æ®é›†
    2. **ä»»åŠ¡ç®¡ç†**: æŸ¥çœ‹æ‰€æœ‰è®­ç»ƒä»»åŠ¡çŠ¶æ€
    3. **æ¨¡å‹å¯¹è¯**: æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
    
    **æç¤º**: é¦–æ¬¡è®­ç»ƒå»ºè®®ä½¿ç”¨è¾ƒå°‘æ ·æœ¬æµ‹è¯•
    """)